<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Causal Discovery example &mdash; DoWhy | An end-to-end library for causal inference  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Identifying Effect using ID Algorithm" href="identifying_effects_using_id_algorithm.html" />
    <link rel="prev" title="Iterating over multiple refutation tests" href="dowhy_refuter_notebook.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> DoWhy | An end-to-end library for causal inference
          </a>
              <div class="version">
                v0.7.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introducing DoWhy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../readme.html">DoWhy | An end-to-end library for causal inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../readme.html#graphical-models-and-potential-outcomes-best-of-both-worlds">Graphical Models and Potential Outcomes: Best of both worlds</a></li>
<li class="toctree-l1"><a class="reference internal" href="../readme.html#four-steps-of-causal-inference">Four steps of causal inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../readme.html#citing-this-package">Citing this package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../readme.html#roadmap">Roadmap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../readme.html#contributing">Contributing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick-Start Tutorial</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorial-causalinference-machinelearning-using-dowhy-econml.html">Tutorial on Causal Inference and its Connections to Machine Learning (Using DoWhy+EconML)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Starter Notebooks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dowhy_simple_example.html">Getting started with DoWhy: A simple example</a></li>
<li class="toctree-l1"><a class="reference internal" href="dowhy_confounder_example.html">Confounding Example: Finding causal effects from observed data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dowhy_estimation_methods.html">DoWhy: Different estimation methods for causal inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="dowhy-simple-iv-example.html">Simple example on using Instrumental Variables method for estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="load_graph_example.html">Different ways to load an input graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="dowhy_interpreter.html">DoWhy: Interpreters for Causal Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="dowhy_causal_api.html">Demo for the DoWhy causal API</a></li>
<li class="toctree-l1"><a class="reference internal" href="do_sampler_demo.html">Do-sampler Introduction</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Case Study Notebooks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="DoWhy-The%20Causal%20Story%20Behind%20Hotel%20Booking%20Cancellations.html">DoWhy-The Causal Story Behind Hotel Booking Cancellations</a></li>
<li class="toctree-l1"><a class="reference internal" href="dowhy_example_effect_of_memberrewards_program.html">Estimating the effect of a Member Rewards program</a></li>
<li class="toctree-l1"><a class="reference internal" href="dowhy_ihdp_data_example.html">DoWhy example on ihdp (Infant Health and Development Program) dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="dowhy_twins_example.html">DoWhy example on Twins dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="dowhy_lalonde_example.html">DoWhy example on the Lalonde dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="dowhy_refutation_testing.html">Applying refutation tests to the Lalonde and IHDP datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="lalonde_pandas_api.html">Lalonde Pandas API Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Notebooks</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="dowhy-conditional-treatment-effects.html">Conditional Average Treatment Effects (CATE) with DoWhy and EconML</a></li>
<li class="toctree-l1"><a class="reference internal" href="dowhy_mediation_analysis.html">Mediation analysis with DoWhy: Direct and Indirect Effects</a></li>
<li class="toctree-l1"><a class="reference internal" href="dowhy_demo_dummy_outcome_refuter.html">A Simple Example on Creating a Custom Refutation Using User-Defined Outcome Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="dowhy_multiple_treatments.html">Estimating effect of multiple treatments</a></li>
<li class="toctree-l1"><a class="reference internal" href="dowhy_refuter_notebook.html">Iterating over multiple refutation tests</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Causal Discovery example</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Utility-function">Utility function</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#Experiments-on-the-Auto-MPG-dataset">Experiments on the Auto-MPG dataset</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#1.-Load-the-data">1. Load the data</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#Causal-Discovery-with-Causal-Discovery-Tool-(CDT)">Causal Discovery with Causal Discovery Tool (CDT)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Estimate-causal-effects-using-Linear-Regression">Estimate causal effects using Linear Regression</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#Experiments-on-the-Sachs-dataset">Experiments on the Sachs dataset</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">1. Load the data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Ground-truth-of-the-causal-graph">Ground truth of the causal graph</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#id2">Causal Discovery with Causal Discovery Tool (CDT)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Estimate-effects-using-Linear-Regression">Estimate effects using Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Graph-Validation">Graph Validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Graph-Refutation">Graph Refutation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="identifying_effects_using_id_algorithm.html">Identifying Effect using ID Algorithm</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../code_repo.html">Code repository &amp; Versions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dowhy.html">dowhy package</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DoWhy | An end-to-end library for causal inference</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="nb_advanced_index.html">&lt;no title&gt;</a> &raquo;</li>
      <li>Causal Discovery example</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/example_notebooks/dowhy_causal_discovery_example.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="Causal-Discovery-example">
<h1>Causal Discovery example<a class="headerlink" href="#Causal-Discovery-example" title="Permalink to this heading"></a></h1>
<p>The goal of this notebook is to show how causal discovery methods can work with DoWhy. We use discovery methods from <a class="reference external" href="https://github.com/FenTechSolutions/CausalDiscoveryToolbox">Causal Discovery Tool (CDT)</a> repo. As we will see, causal discovery methods are not fool-proof and there is no guarantee that they will recover the correct causal graph. Even for the simple examples below, there is a large variance in results. These methods, however, may be combined usefully with domain knowledge to
construct the final causal graph.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import dowhy
from dowhy import CausalModel

from rpy2.robjects import r as R
%load_ext rpy2.ipython

import numpy as np
import pandas as pd
import graphviz
import networkx as nx

np.set_printoptions(precision=3, suppress=True)
np.random.seed(0)
</pre></div>
</div>
</div>
<section id="Utility-function">
<h2>Utility function<a class="headerlink" href="#Utility-function" title="Permalink to this heading"></a></h2>
<p>We define a utility function to draw the directed acyclic graph.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def make_graph(adjacency_matrix, labels=None):
    idx = np.abs(adjacency_matrix) &gt; 0.01
    dirs = np.where(idx)
    d = graphviz.Digraph(engine=&#39;dot&#39;)
    names = labels if labels else [f&#39;x{i}&#39; for i in range(len(adjacency_matrix))]
    for name in names:
        d.node(name)
    for to, from_, coef in zip(dirs[0], dirs[1], adjacency_matrix[idx]):
        d.edge(names[from_], names[to], label=str(coef))
    return d

def str_to_dot(string):
    &#39;&#39;&#39;
    Converts input string from graphviz library to valid DOT graph format.
    &#39;&#39;&#39;
    graph = string.replace(&#39;\n&#39;, &#39;;&#39;).replace(&#39;\t&#39;,&#39;&#39;)
    graph = graph[:9] + graph[10:-2] + graph[-1] # Removing unnecessary characters from string
    return graph
</pre></div>
</div>
</div>
</section>
</section>
<section id="Experiments-on-the-Auto-MPG-dataset">
<h1>Experiments on the Auto-MPG dataset<a class="headerlink" href="#Experiments-on-the-Auto-MPG-dataset" title="Permalink to this heading"></a></h1>
<p>In this section, we will use a dataset on the technical specification of cars. The dataset is downloaded from UCI Machine Learning Repository. The dataset contains 9 attributes and 398 instances. We do not know the true causal graph for the dataset and will use CDT to discover it. The causal graph obtained will then be used to estimate the causal effect.</p>
<section id="1.-Load-the-data">
<h2>1. Load the data<a class="headerlink" href="#1.-Load-the-data" title="Permalink to this heading"></a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>data_mpg = pd.read_csv(&#39;http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data-original&#39;,
                   delim_whitespace=True, header=None,
                   names = [&#39;mpg&#39;, &#39;cylinders&#39;, &#39;displacement&#39;,
                            &#39;horsepower&#39;, &#39;weight&#39;, &#39;acceleration&#39;,
                            &#39;model year&#39;, &#39;origin&#39;, &#39;car name&#39;])
data_mpg.dropna(inplace=True)
data_mpg.drop([&#39;model year&#39;, &#39;origin&#39;, &#39;car name&#39;], axis=1, inplace=True)
print(data_mpg.shape)
data_mpg.head()
</pre></div>
</div>
</div>
</section>
</section>
<section id="Causal-Discovery-with-Causal-Discovery-Tool-(CDT)">
<h1>Causal Discovery with Causal Discovery Tool (CDT)<a class="headerlink" href="#Causal-Discovery-with-Causal-Discovery-Tool-(CDT)" title="Permalink to this heading"></a></h1>
<p>We use the CDT library to perform causal discovery on the Auto-MPG dataset. We use three methods for causal discovery here -LiNGAM, PC and GES. These methods are widely used and do not take much time to run. Hence, these are ideal for an introduction to the topic. Other neural network based methods are also available in CDT and the users are encouraged to try them out by themselves.</p>
<p>The documentation for the methods used are as follows: - LiNGAM <a class="reference external" href="https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/_modules/cdt/causality/graph/LiNGAM.html">[link]</a> - PC <a class="reference external" href="https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/_modules/cdt/causality/graph/PC.html">[link]</a> - GES <a class="reference external" href="https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/_modules/cdt/causality/graph/GES.html">[link]</a></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from cdt.causality.graph import LiNGAM, PC, GES

graphs = {}
labels = [f&#39;{col}&#39; for i, col in enumerate(data_mpg.columns)]
functions = {
    &#39;LiNGAM&#39; : LiNGAM,
    &#39;PC&#39; : PC,
    &#39;GES&#39; : GES,
}

for method, lib in functions.items():
    obj = lib()
    output = obj.predict(data_mpg)
    adj_matrix = nx.to_numpy_matrix(output)
    adj_matrix = np.asarray(adj_matrix)
    graph_dot = make_graph(adj_matrix, labels)
    graphs[method] = graph_dot

# Visualize graphs
for method, graph in graphs.items():
    print(&quot;Method : %s&quot;%(method))
    display(graph)
</pre></div>
</div>
</div>
<p>As you can see, no two methods agree on the graphs. PC and GES effectively produce an undirected graph whereas LiNGAM produces a directed graph. We use only the LiNGAM method in the next section.</p>
<section id="Estimate-causal-effects-using-Linear-Regression">
<h2>Estimate causal effects using Linear Regression<a class="headerlink" href="#Estimate-causal-effects-using-Linear-Regression" title="Permalink to this heading"></a></h2>
<p>Now let us see whether these differences in the graphs also lead to signficant differences in the causal estimate of effect of <em>mpg</em> on <em>weight</em>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>for method, graph in graphs.items():
        if method != &quot;LiNGAM&quot;:
            continue
        print(&#39;\n*****************************************************************************\n&#39;)
        print(&quot;Causal Discovery Method : %s&quot;%(method))

        # Obtain valid dot format
        graph_dot = str_to_dot(graph.source)

        # Define Causal Model
        model=CausalModel(
                data = data_mpg,
                treatment=&#39;mpg&#39;,
                outcome=&#39;weight&#39;,
                graph=graph_dot)

        # Identification
        identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)
        print(identified_estimand)

        # Estimation
        estimate = model.estimate_effect(identified_estimand,
                                        method_name=&quot;backdoor.linear_regression&quot;,
                                        control_value=0,
                                        treatment_value=1,
                                        confidence_intervals=True,
                                        test_significance=True)
        print(&quot;Causal Estimate is &quot; + str(estimate.value))
</pre></div>
</div>
</div>
<p>As mentioned earlier, due to the absence of directed edges, no backdoor, instrmental or frontdoor variables can be found out for PC and GES. Thus, causal effect estimation is not possible for these methods. However, LiNGAM does discover a DAG and hence, its possible to output a causal estimate for LiNGAM. The estimate is still pretty far from the original estimate of -70.466 (which can be calculated from the graph).</p>
</section>
</section>
<section id="Experiments-on-the-Sachs-dataset">
<h1>Experiments on the Sachs dataset<a class="headerlink" href="#Experiments-on-the-Sachs-dataset" title="Permalink to this heading"></a></h1>
<p>The dataset consists of the simultaneous measurements of 11 phosphorylated proteins and phospholipids derived from thousands of individual primary immune system cells, subjected to both general and specific molecular interventions (Sachs et al., 2005).</p>
<p>The specifications of the dataset are as follows - - Number of nodes: 11 - Number of arcs: 17 - Number of parameters: 178 - Average Markov blanket size: 3.09 - Average degree: 3.09 - Maximum in-degree: 3 - Number of instances: 7466</p>
<p>The original causal graph is known for the Sachs dataset and we compare the original graph with the ones discovered using CDT in this section.</p>
<section id="id1">
<h2>1. Load the data<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from cdt.data import load_dataset
data_sachs, graph_sachs = load_dataset(&quot;sachs&quot;)

data_sachs.dropna(inplace=True)
print(data_sachs.shape)
data_sachs.head()
</pre></div>
</div>
</div>
</section>
<section id="Ground-truth-of-the-causal-graph">
<h2>Ground truth of the causal graph<a class="headerlink" href="#Ground-truth-of-the-causal-graph" title="Permalink to this heading"></a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>labels = [f&#39;{col}&#39; for i, col in enumerate(data_sachs.columns)]
adj_matrix = nx.to_numpy_matrix(graph_sachs)
adj_matrix = np.asarray(adj_matrix)
graph_dot = make_graph(adj_matrix, labels)
display(graph_dot)
</pre></div>
</div>
</div>
</section>
</section>
<section id="id2">
<h1>Causal Discovery with Causal Discovery Tool (CDT)<a class="headerlink" href="#id2" title="Permalink to this heading"></a></h1>
<p>We use the CDT library to perform causal discovery on the Auto-MPG dataset. We use three methods for causal discovery here -LiNGAM, PC and GES. These methods are widely used and do not take much time to run. Hence, these are ideal for an introduction to the topic. Other neural network based methods are also available in CDT and the users the encourages to try them out by themselves.</p>
<p>The documentation for the methods used in as follows: - LiNGAM <a class="reference external" href="https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/_modules/cdt/causality/graph/LiNGAM.html">[link]</a> - PC <a class="reference external" href="https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/_modules/cdt/causality/graph/PC.html">[link]</a> - GES <a class="reference external" href="https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/_modules/cdt/causality/graph/GES.html">[link]</a></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from cdt.causality.graph import LiNGAM, PC, GES

graphs = {}
graphs_nx = {}
labels = [f&#39;{col}&#39; for i, col in enumerate(data_sachs.columns)]
functions = {
    &#39;LiNGAM&#39; : LiNGAM,
    &#39;PC&#39; : PC,
    &#39;GES&#39; : GES,
}

for method, lib in functions.items():
    obj = lib()
    output = obj.predict(data_sachs)
    graphs_nx[method] = output
    adj_matrix = nx.to_numpy_matrix(output)
    adj_matrix = np.asarray(adj_matrix)
    graph_dot = make_graph(adj_matrix, labels)
    graphs[method] = graph_dot

# Visualize graphs
for method, graph in graphs.items():
    print(&quot;Method : %s&quot;%(method))
    display(graph)
</pre></div>
</div>
</div>
<p>As you can see, no two methods agree on the graphs. Next we study the causal effects of these different graphs</p>
<section id="Estimate-effects-using-Linear-Regression">
<h2>Estimate effects using Linear Regression<a class="headerlink" href="#Estimate-effects-using-Linear-Regression" title="Permalink to this heading"></a></h2>
<p>Now let us see whether these differences in the graphs also lead to signficant differences in the causal estimate of effect of <em>PIP2</em> on <em>PKC</em>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>for method, graph in graphs.items():
        if method != &quot;LiNGAM&quot;:
            continue
        print(&#39;\n*****************************************************************************\n&#39;)
        print(&quot;Causal Discovery Method : %s&quot;%(method))

        # Obtain valid dot format
        graph_dot = str_to_dot(graph.source)

        # Define Causal Model
        model=CausalModel(
                data = data_sachs,
                treatment=&#39;PIP2&#39;,
                outcome=&#39;PKC&#39;,
                graph=graph_dot)

        # Identification
        identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)
        print(identified_estimand)

        # Estimation
        estimate = model.estimate_effect(identified_estimand,
                                        method_name=&quot;backdoor.linear_regression&quot;,
                                        control_value=0,
                                        treatment_value=1,
                                        confidence_intervals=True,
                                        test_significance=True)
        print(&quot;Causal Estimate is &quot; + str(estimate.value))
</pre></div>
</div>
</div>
<p>From the causal estimates obtained, it can be seen that the three estimates differ in different aspects. The graph obtained using LiNGAM contains a backdoor path and instrumental variables. On the other hand, the graph obtained using PC contains a backdoor path and a frontdoor path. However, despite these differences, both obtain the same mean causal estimate.</p>
<p>The graph obtained using GES contains only a backdoor path with different backdoor variables and obtains a different causal estimate than the first two cases.</p>
</section>
<section id="Graph-Validation">
<h2>Graph Validation<a class="headerlink" href="#Graph-Validation" title="Permalink to this heading"></a></h2>
<p>We compare the graphs obtained with the true causal graph using the causal discovery methods using 2 graph distance metrics - Structural Hamming Distance (SHD) and Structural Intervention Distance (SID). SHD between two graphs is, in simple terms, the number of edge insertions, deletions or flips in order to transform one graph to another graph. SID, on the other hand, is based on a graphical criterion only and quantifies the closeness between two DAGs in terms of their corresponding causal
inference statements.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from cdt.metrics import SHD, SHD_CPDAG, SID, SID_CPDAG
from numpy.random import randint

for method, graph in graphs_nx.items():
    print(&quot;***********************************************************&quot;)
    print(&quot;Method: %s&quot;%(method))
    tar, pred = graph_sachs, graph
    print(&quot;SHD_CPDAG = %f&quot;%(SHD_CPDAG(tar, pred)))
    print(&quot;SHD = %f&quot;%(SHD(tar, pred, double_for_anticausal=False)))
    print(&quot;SID_CPDAG = [%f, %f]&quot;%(SID_CPDAG(tar, pred)))
    print(&quot;SID = %f&quot;%(SID(tar, pred)))
</pre></div>
</div>
</div>
<p>The graph similarity metrics show that the scores are the lowest for the LiNGAM method of graph extraction. Hence, of the three methods used, LiNGAM provides the graph that is most similar to the original graph.</p>
</section>
<section id="Graph-Refutation">
<h2>Graph Refutation<a class="headerlink" href="#Graph-Refutation" title="Permalink to this heading"></a></h2>
<p>Here, we use the same SHD and SID metric to find out how different the discovered graph are from each other.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import itertools
from numpy.random import randint
from cdt.metrics import SHD, SHD_CPDAG, SID, SID_CPDAG

# Find combinations of pair of methods to compare
combinations = list(itertools.combinations(graphs_nx, 2))

for pair in combinations:
    print(&quot;***********************************************************&quot;)
    graph1 = graphs_nx[pair[0]]
    graph2 = graphs_nx[pair[1]]
    print(&quot;Methods: %s and %s&quot;%(pair[0], pair[1]))
    print(&quot;SHD_CPDAG = %f&quot;%(SHD_CPDAG(graph1, graph2)))
    print(&quot;SHD = %f&quot;%(SHD(graph1, graph2, double_for_anticausal=False)))
    print(&quot;SID_CPDAG = [%f, %f]&quot;%(SID_CPDAG(graph1, graph2)))
    print(&quot;SID = %f&quot;%(SID(graph1, graph2)))
</pre></div>
</div>
</div>
<p>The values for the metrics show how different the graphs are from each other. A higher distance value implies that the difference between the graphs is more.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="dowhy_refuter_notebook.html" class="btn btn-neutral float-left" title="Iterating over multiple refutation tests" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="identifying_effects_using_id_algorithm.html" class="btn btn-neutral float-right" title="Identifying Effect using ID Algorithm" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, PyWhy contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: v0.7.1
    <span class="fa fa-caret-down"></span>
  </span>
    <div class="rst-other-versions">
        <dl>
            <dt>Releases</dt>
            <dd><a href="dowhy_causal_discovery_example.html">v0.7.1</a></dd>
        </dl>
        <dl>
            <dt>Branches</dt>
            <dd><a href="../../master/index.html">master</a></dd>
        </dl>
    </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>