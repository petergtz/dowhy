<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Ranking of estimation methods for a given dataset &mdash; DoWhy | An end-to-end library for causal inference  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> DoWhy | An end-to-end library for causal inference
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/intro.html">Introduction to DoWhy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="dowhy_simple_example.html">Quick-start notebook</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/comparison.html">Comparison to other packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/cite.html">Citing this package</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/causality_intro.html">Introduction to causality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/four_steps_causality.html">Effect inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="nb_index.html">Example notebooks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials/Case studies</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorial-causalinference-machinelearning-using-dowhy-econml.html">CATE estimation with DoWhy+EconML</a></li>
<li class="toctree-l1"><a class="reference internal" href="dowhy_example_effect_of_memberrewards_program.html">Effect of membership rewards program</a></li>
<li class="toctree-l1"><a class="reference internal" href="DoWhy-The%20Causal%20Story%20Behind%20Hotel%20Booking%20Cancellations.html">Understanding hotel booking cancellations</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GCM-based inference (Experimental)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gcm/user_guide/index.html">GCMs User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="gcm_basic_example.html">Basic Example for GCM-Based Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="rca_microservice_architecture.html">Root cause analysis (RCA) of latencies in a microservice architecture</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contributing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contributing/index.html">Contributing to DoWhy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../code_repo.html">Code repository &amp; Versions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dowhy.html">dowhy package</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DoWhy | An end-to-end library for causal inference</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Ranking of estimation methods for a given dataset</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/example_notebooks/dowhy_ranking_methods.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="Ranking-of-estimation-methods-for-a-given-dataset">
<h1>Ranking of estimation methods for a given dataset<a class="headerlink" href="#Ranking-of-estimation-methods-for-a-given-dataset" title="Permalink to this heading"></a></h1>
<p>We illustrate the comparison of various estimation methods for a given datasets by ranking them according to their performance against refutation tests accounting for both the observed unmodelled confounding error and unobserved confounding error.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Importing all the required libraries
import sys
import argparse
import xgboost
import numpy as np
import pandas as pd
import os
import pdb
import random
from xgboost import XGBRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LassoCV
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor
from sklearn.linear_model import LogisticRegressionCV
from sklearn.metrics import mean_absolute_error
from dowhy import CausalModel
from datetime import datetime
from collections import namedtuple

import statsmodels.api as sm
from sklearn import linear_model

import dowhy
from dowhy.utils import dgp
from dowhy.utils.dgps.linear_dgp import LinearDataGeneratingProcess
from dowhy import CausalModel
from datetime import datetime
from collections import namedtuple
from dowhy.causal_refuters.add_unobserved_common_cause import AddUnobservedCommonCause

import matplotlib.pyplot as plt
import matplotlib.lines as mlines
import matplotlib.transforms as mtransforms

# Config dict to set the logging level
import logging.config
DEFAULT_LOGGING = {
    &#39;version&#39;: 1,
    &#39;disable_existing_loggers&#39;: False,
    &#39;loggers&#39;: {
        &#39;&#39;: {
            &#39;level&#39;: &#39;WARN&#39;,
        },
    }
}

logging.config.dictConfig(DEFAULT_LOGGING)
# Disabling warnings output
import warnings
from sklearn.exceptions import DataConversionWarning, ConvergenceWarning
warnings.filterwarnings(action=&#39;ignore&#39;, category=DataConversionWarning)
warnings.filterwarnings(action=&#39;ignore&#39;, category=ConvergenceWarning)
<br/></pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def convert_singleton_to_float(arr):
    &#39;&#39;&#39;Helper function.&#39;&#39;&#39;
    array = []

    if len(arr) == 1 and type(arr[0]) != np.ndarray:
        return arr[0]

    for element in arr:
        while type(element) == np.ndarray or isinstance(element, list) :
            if len(element) &gt; 1:
                raise ValueError(&quot;This script only accepts one value for the refute&quot;)
            element = element[0]
        array.append(element)

    return array
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def ensure_dir(file_path):
    directory = os.path.dirname(file_path)
    if not os.path.exists(directory):
        os.makedirs(directory)

RESULTSFOLDER = &quot;results/&quot;
ensure_dir(RESULTSFOLDER)
# Create the estimator named tuple to wrap the name and properties
Estimator = namedtuple(&#39;Estimator&#39;, [&#39;name&#39;,&#39;params&#39;])
Refuter = namedtuple(&#39;Refuter&#39;, [&#39;name&#39;,&#39;params&#39;])

class Experiment():
    &#39;&#39;&#39;
    Class to define the experiment setup to compare a list of estimators across a list of refuters for the given dataset.
    &#39;&#39;&#39;
    def __init__(self, **kwargs):
        self.experiment_name = kwargs[&#39;experiment_name&#39;]
        self.experiment_id = kwargs[&#39;experiment_id&#39;]
        self.num_experiments = kwargs[&#39;num_experiments&#39;]
        self.sample_sizes = kwargs[&#39;sample_sizes&#39;]
        self.dgps = kwargs[&#39;dgps&#39;]
        self.estimators = kwargs[&#39;estimators&#39;]
        self.refuters = kwargs[&#39;refuters&#39;]
        self.results = []
        self.simulate_unobserved_confounding = kwargs[&quot;simulate_unobserved_confounding&quot;]

        # Handle input errors in sample_sizes
        if isinstance(self.sample_sizes, list) == False:
            if type(self.sample_sizes) != int:
                raise ValueError(&#39;The input to &quot;sample_sizes&quot; should be an int or a list&#39;)
            else:
                self.sample_sizes = [self.sample_sizes]

        # Handle input errors in DGPs
        if isinstance(self.dgps, list) == False:
            if isinstance(self.dgps, DataGeneratingProcess) == False:
                raise ValueError(&#39;The input to &quot;dgps&quot; should be a list or a subclass of &quot;DataGeneratingProcess&quot;&#39;)
            else:
                self.dgps = [self.dgps]

        # Handle inputs errors in estimators
        if isinstance(self.estimators, list) == False:
            if isinstance(self.estimators, Estimator) == False:
                raise ValueError(&#39;The input to &quot;estimators&quot; should be a list or an Estimator namedtuple&#39;)
            else:
                self.estimators = [self.estimators]

        # Handle input errors in refuters
        if isinstance(self.refuters, list) == False:
            if isinstance(self.refuters, Refuter) == False:
                raise ValueError(&#39;The input to &quot;refuters&quot; should be a list of a Refuter namedtuple&#39;)
            else:
                self.refuters = [self.refuters]

    def experiment(self):
        print(&quot;\n\nRunning Experiment:&quot;,self.experiment_name + &#39;_&#39; + str(self.experiment_id) )

        for exp in range(self.num_experiments):
            print(&quot;\n\nRunning Experiment Number:&quot;,exp)

            for sample_size in self.sample_sizes:

                print(&quot;\n\nCurrent Sample Size:&quot;,sample_size)

                for dgp in self.dgps:
                    print(&quot;\n\nThe current DGP:&quot;)
                    print(dgp)
                    estimates = []
                    estimate_values = []
                    estimated_effect = []
                    new_effect = []
                    p_value = []
                    data = dgp.generate_data(sample_size)
                    print(&quot;printing data shape&quot;)
                    print(data.values.shape)
                    print(dgp.true_value)
                    print(&quot;check&quot;)
                    if dgp.treatment_is_binary:
                        data[dgp.treatment] = data[dgp.treatment].astype(bool)
                    #k = len(dgp.confounder)-4
                    #confounder_list = random.sample(dgp.confounder, k)
                    confounder_list = [&#39;w2&#39;,&#39;w3&#39;]


                    s = set(confounder_list)
                    unobserved_confounders = [x for x in dgp.confounder if x not in s]
                    df_unobserved_confounders = pd.DataFrame(data = data[[c for c in data.columns if c in unobserved_confounders]])

                    df_unobserved_confounders.to_csv(&quot;results/unobserved_confounders.csv&quot;)
                    print(&quot;printing length of confounder list:&quot;, len(confounder_list))
                    print(&quot;printing confounder list:&quot;, confounder_list)



                    print(&quot;data columns&quot;)

                    print(&quot;data columns&quot;, data.columns)
                    model = CausalModel(
                        data = data,
                        treatment = dgp.treatment,
                        outcome = dgp.outcome,
                        common_causes = confounder_list,
                        effect_modifiers = dgp.effect_modifier
                    )
                    model.view_model()
                    from IPython.display import Image, display
                    display(Image(filename=&quot;causal_model.png&quot;))

                    identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)

                    print(&quot;identified_estimand:&quot;, identified_estimand)
                    #print(&quot;identified_estimand:&quot;, identified_estimand)
                    print(&quot;\n\nRunning the estimators:\n&quot;)
                    for estimator in self.estimators:
                        print(&quot;The current estimator:&quot;, estimator)
                        print(&quot;estimator.params&quot;, estimator.params)
                        estimate = model.estimate_effect(
                            identified_estimand,
                            method_name = estimator.name,
                            method_params = estimator.params
                        )
                        print(&quot;printing estimate&#39;s type&quot;)
                        print(type(estimate))
                        estimates.append(estimate)
                        estimate_values.append(estimate.value)
                    estimate_values = convert_singleton_to_float(estimate_values)
                    print(&quot;estimate_values&quot;, estimate_values)
                    print(&quot;\n\nRunning the refuters:\n&quot;)
                    for refuter in self.refuters:
                        print(&quot;The current refuter:&quot;, refuter)

                        for estimate in estimates:
                            if self.simulate_unobserved_confounding == True:
                                print(&quot;********%%%%%%%%%$$$$$&amp;&amp;^**^^^^*^*^*&quot;)
                                if refuter.name == &#39;dummy_outcome_refuter&#39;:
                                    add_unobserved_confounder = AddUnobservedCommonCause(data, identified_estimand, estimate)
                                    print(&quot;add_unobserved_confounder&quot;, add_unobserved_confounder)
                                    unobserved_confounder_values = add_unobserved_confounder.include_simulated_confounder(convergence_threshold = 0.11, c_star_max = 1500)
                                    refuter.params[&#39;unobserved_confounder_values&#39;] = unobserved_confounder_values
                                    print(&#39;refuter.params&#39;, refuter.params)
                            refute = model.refute_estimate(
                                identified_estimand,
                                estimate,
                                method_name = refuter.name,
                                **refuter.params,



                            )
                            print(&quot;printing refute&#39;s type&quot;)
                            print(type(refute))
                            if(refuter.name == &#39;dummy_outcome_refuter&#39;):
                                refute = refute[0]
                            if refute.refutation_result is not None:
                                p_value.append(refute.refutation_result[&#39;p_value&#39;])
                            else:
                                p_value.append(None)

                            estimated_effect.append(refute.estimated_effect)
                            #print(&quot;refute.estimate_effect()&quot;, refute.estimate_effect())
                            new_effect.append(refute.new_effect)

                    estimated_effect = convert_singleton_to_float(estimated_effect)
                    new_effect = convert_singleton_to_float(new_effect)
                    p_value = convert_singleton_to_float(p_value)
                    true_value = convert_singleton_to_float(dgp.true_value)

                    print(&quot;estimated effect&quot;, estimated_effect)
                    print(&quot;new_effect&quot;, new_effect)
                    print(&quot;p_value&quot;, p_value)
                    print(&quot;true value&quot;, true_value)
                    self.results.append([exp, sample_size, dgp.NAME, *estimate_values, *estimated_effect, *new_effect, *p_value, true_value])


        print(&quot;\n\nCompleted all experiments. Saving the data...&quot;)

        COLUMNS = [&#39;EXPERIMENT&#39;, &#39;SAMPLE_SIZE&#39;, &#39;DGP&#39;]
        RESULT_CATEGORIES = [&#39;ESTIMATED_EFFECT&#39;, &#39;NEW_EFFECT&#39;, &#39;P_VALUE&#39;]
        estimator_names = [estimator.name for estimator in self.estimators]
        refuter_names = [refuter.name for refuter in self.refuters]

        for estimator_name in estimator_names:
            COLUMNS += [&#39;ORIGINAL_ESTIMATE&#39;+ &#39;:&#39; + estimator_name]

        for result_category in RESULT_CATEGORIES:
            for refuter_name in refuter_names:
                for estimator_name in estimator_names:
                    COLUMNS += [refuter_name + &#39;:&#39; + estimator_name + &#39;:&#39; + result_category]

        COLUMNS += [&#39;TRUE_VALUE&#39;]

        csv_file = RESULTSFOLDER + self.experiment_name+ &#39;_&#39; + str(self.experiment_id) + &#39;_&#39; + str(datetime.utcnow().date()) + &#39;_data.csv&#39;
        onlyres_csv_file = RESULTSFOLDER + &quot;onlyres_&quot;+ self.experiment_name+ &#39;_&#39; + str(self.experiment_id) + &#39;_&#39; + str(datetime.utcnow()) + &#39;_data.csv&#39;
        self.results = pd.DataFrame(data=self.results,columns=COLUMNS)
        self.results.to_csv(csv_file.replace(&quot; &quot;, &quot;&quot;), index=False)

        print(&quot;Data has been saved in &quot;,csv_file)

        return csv_file
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>#Defining the Data Generating Process
ldgp = LinearDataGeneratingProcess(treatment=[&#39;t1&#39;], outcome=[&#39;y&#39;], confounder=[&#39;w1&#39;,&#39;w2&#39;, &#39;w3&#39;,&#39;w4&#39;,&#39;w5&#39;,&#39;w6&#39;], effect_modifier=[&#39;x1&#39;,&#39;x2&#39;], seed=None, treatment_is_binary=True)

#Defining the sample size
sample_size = 1000

dgp_dict = {&#39;ldgp&#39;:ldgp}
dgp_list = []
dgp_list.append( dgp_dict[&#39;ldgp&#39;] )


# Create a namedtuple to store the name of the estimator and the parameters passed
estimator_list = [&quot;backdoor.linear_regression&quot;,
                  #&quot;backdoor.propensity_score_stratification&quot;,
                  &quot;backdoor.propensity_score_matching&quot;,
                  &quot;backdoor.propensity_score_weighting&quot;,
                  &quot;backdoor.econml.dml.DML&quot;,
                  &quot;backdoor.econml.dr.LinearDRLearner&quot;,
                  #&quot;backdoor.econml.metalearners.TLearner&quot;,
                  #&quot;backdoor.econml.metalearners.XLearner&quot;,
                  #&quot;backdoor.causalml.inference.meta.LRSRegressor&quot;,
                  #&quot;backdoor.causalml.inference.meta.XGBTRegressor&quot;,
                  #&quot;backdoor.causalml.inference.meta.MLPTRegressor&quot;,
                  #&quot;backdoor.causalml.inference.meta.BaseXRegressor&quot;
                ]
method_params= [    None,
                    #None,
                    { &quot;init_params&quot;:{} },
                    { &quot;init_params&quot;:{} },
                    {&quot;init_params&quot;:{&#39;model_y&#39;:GradientBoostingRegressor(),
                                    &#39;model_t&#39;: GradientBoostingRegressor(),
                                    &quot;model_final&quot;:LassoCV(fit_intercept=False),
                                    &#39;featurizer&#39;:PolynomialFeatures(degree=1, include_bias=True)},
                     &quot;fit_params&quot;:{}},
                    {&quot;init_params&quot;:{ &#39;model_propensity&#39;: LogisticRegressionCV(cv=3, solver=&#39;lbfgs&#39;, multi_class=&#39;auto&#39;),
                        },
                    &quot;fit_params&quot;:{}
                    },
                    &#39;&#39;&#39;{&quot;init_params&quot;: {&#39;models&#39;: GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(sample_size/100))
                                    },
                    &quot;fit_params&quot;:{}
                    },
                    {&quot;init_params&quot;:{&#39;models&#39;: GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(sample_size/100)),
                        &#39;propensity_model&#39;: RandomForestClassifier(n_estimators=100, max_depth=6,
                                                                              min_samples_leaf=int(sample_size/100))
                        },
                     &quot;fit_params&quot;:{}
                    },
                    {&quot;init_params&quot;:{},},
                    {&quot;init_params&quot;:{
                        &#39;learner&#39;:XGBRegressor()
                        }
                    }&#39;&#39;&#39;
                ]
estimator_tuples = []
refuter_tuples = []

refuter_list = [&#39;dummy_outcome_refuter&#39;]
refuter_params = [{&#39;num_simulations&#39;:5,&#39;transformation_list&#39;: [(&#39;random_forest&#39;,{&#39;n_estimators&#39;:100, &#39;max_depth&#39;:6})], &#39;true_causal_effect&#39;:(lambda x:0.5)}]


# Iterate through the names and parameters to create a list of namedtuples
for name, param in zip(estimator_list,method_params):
    estimator_tuples.append(Estimator._make([name, param]))

for name, param in zip(refuter_list, refuter_params):
    refuter_tuples.append(Refuter._make([name, param]))
<br/></pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def plot_MAEs(res):
    true_value_column = res.columns[-1]
    estimate_columns=res.columns[3:-1]
    #print(estimate_columns)
    #print(type(estimate_columns))
    estimate_columns.append(pd.Index(res[&quot;TRUE_VALUE&quot;]))
    #print(estimate_columns)
    fig, ax = plt.subplots()
    MAE ={}
    for colname in estimate_columns:
        if colname not in (&#39;ORIGINAL_ESTIMATE:backdoor.propensity_score_weighting&#39;,):
                           #&#39;ORIGINAL_ESTIMATE:backdoor.econml.metalearners.TLearner&#39;):
            plt.plot(res[colname], res[&quot;TRUE_VALUE&quot;], marker=&#39;o&#39;, linestyle=&quot;None&quot;, label=colname)
            &quot;Mean Absolute Error (MAE): {}&quot;.format(mean_absolute_error(res[colname], res[&quot;TRUE_VALUE&quot;]))
            MAE[colname] = mean_absolute_error(res[colname], res[&quot;TRUE_VALUE&quot;])
    fig.suptitle(&#39;Calibration plot showing the accuracy of different causal estimators [P(T=1)=0.9]&#39;)
    ax.set_xlabel(&#39;Estimated effect&#39;)
    ax.set_ylabel(&#39;True causal effect&#39;)
    plt.legend(loc=&#39;upper center&#39;, bbox_to_anchor=(0.5, -0.20),
              fancybox=True, shadow=True, ncol=2)
    plt.show()
    print(&quot;Printing MAE of various estimates: &quot;)
    MAE_values = {k: v for k, v in sorted(MAE.items(), key=lambda item: item[1], reverse = True)}
    for k,v in MAE_values.items():
        print(k, v)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def plot_estimators_and_refuters(refuter, estimator):
    x = list(res[&#39;EXPERIMENT&#39;])
    y1 = list(res[refuter+&#39;:&#39;+estimator+&#39;:ESTIMATED_EFFECT&#39;])
    y2 = list(res[refuter+&#39;:&#39;+estimator+&#39;:NEW_EFFECT&#39;])
    #print(res[&#39;TRUE_VALUE&#39;])
    y3 = list(res[&#39;TRUE_VALUE&#39;])
    y4 = list(res[refuter+&#39;:&#39;+estimator+&#39;:P_VALUE&#39;])
    plt.scatter(x, y1, c =&quot;blue&quot;, label = &quot;Estimated Effect&quot;)
    plt.scatter(x, y2, c =&quot;red&quot;, label = &quot;New Effect&quot;)
    plt.scatter(x, y3, c =&quot;green&quot;, label = &quot;True Value&quot;)
    plt.scatter(x, y4, c =&quot;yellow&quot;, label = &quot;P Value&quot;)
    plt.xlabel(&quot;EXPERIMENT&quot;)
    plt.ylabel(&quot;EFFECT&quot;)
    legend = plt.legend(loc=4, fontsize=&#39;small&#39;, fancybox=True)
    plt.title(estimator)
    plt.show()
    plt.savefig(estimator+&#39;.png&#39;)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def plot_deviations(estimator_list, deviation_list):
    plt.scatter(estimator_list, deviation_list)
    plt.xticks(estimator_list, estimator_list, rotation=&#39;vertical&#39;)
    plt.show()
</pre></div>
</div>
</div>
</section>
<section id="Observed-unmodelled-confounding-error">
<h1>Observed unmodelled confounding error<a class="headerlink" href="#Observed-unmodelled-confounding-error" title="Permalink to this heading"></a></h1>
<p>For each estimator, we use dummy outcome refuter to check the observed unmodelled confounding error for each estimator. That is, we run the refutation test for each estimator only on the observed confounders and analyse what amount of confounding error is present unmodelled amongst the observed variables.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Define the properties of the experiment
# The name of the experiment
# The experiment ID
# The number of experiments to be run with the SAME parameters
# The size of the samples to be run
# The list of DGPs to be run
# The list of estimators
observed_confounding_error = Experiment(
    experiment_name=&#39;Test&#39;,
    experiment_id=&#39;1&#39;,
    num_experiments=10, # 10
    sample_sizes=sample_size,
    dgps=dgp_list,
    estimators=estimator_tuples,
    refuters=refuter_tuples,
    simulate_unobserved_confounding = False
)

# Run the experiment
res = pd.read_csv(observed_confounding_error.experiment())
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>#PLOT
#This plot shows the Mean Absolute Error of the Orginal Estimate from the true value and of the New Effect from
#the expected value for each estimator.
plot_MAEs(res)
</pre></div>
</div>
</div>
<section id="Ranking-based-on-Original-Estimate">
<h2>Ranking based on Original Estimate<a class="headerlink" href="#Ranking-based-on-Original-Estimate" title="Permalink to this heading"></a></h2>
<p>The Original Estimate is calculated in the presence of the True Value (that is, the ground truth). However in many real life datasets, the ground truth may not be known. Hence, we want the ranking produced by our refutation tests to be in coherence with that obtained from the Original Estimates. According to the Original Estimate values, the ranking of the estimators should be as follows (the method with the least MAE should get the best rank): 1. DMLCateEstimator 2. LinearRegression 3.
LinearDRLearner 4. Propensity Score Matching</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>estimator_list = [&quot;backdoor.linear_regression&quot;,
                  #&quot;backdoor.propensity_score_stratification&quot;,
                  &quot;backdoor.propensity_score_matching&quot;,
                  &quot;backdoor.econml.dml.DML&quot;,
                  &quot;backdoor.econml.dr.LinearDRLearner&quot;,
                  #&quot;backdoor.econml.metalearners.TLearner&quot;,
                  #&quot;backdoor.econml.metalearners.XLearner&quot;,
                  #&quot;backdoor.causalml.inference.meta.LRSRegressor&quot;,
                  #&quot;backdoor.causalml.inference.meta.XGBTRegressor&quot;,
                  #&quot;backdoor.causalml.inference.meta.MLPTRegressor&quot;,
                  #&quot;backdoor.causalml.inference.meta.BaseXRegressor&quot;
                ]
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>#This plot shows the deviation of the original estimate, the new effect and the estimated effect from the true value
refuter = &#39;dummy_outcome_refuter&#39;
deviation_list = []
for estimator in estimator_list:
    plot_estimators_and_refuters(refuter, estimator)
    avg_deviation = ((res[refuter+&#39;:&#39;+estimator+&#39;:NEW_EFFECT&#39;]).sum(axis=0))
    print(avg_deviation)
    deviation_list.append(avg_deviation)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_deviations(estimator_list, deviation_list)
for i in range(len(estimator_list)):
    print(estimator_list[i] +&quot;: &quot;+ str(deviation_list[i]))
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>{k: v for k, v in sorted(zip(estimator_list, deviation_list), key=lambda item: item[1], reverse = True)}
<br/></pre></div>
</div>
</div>
</section>
<section id="Ranking-based-on-New-Effect-(Refutatation-results)">
<h2>Ranking based on New Effect (Refutatation results)<a class="headerlink" href="#Ranking-based-on-New-Effect-(Refutatation-results)" title="Permalink to this heading"></a></h2>
<p>The ranking based on absolute value of deviations is : 1. Propensity Score Matching 2. Linear DR Learner 3. DML CATE Estimator 4. Linear Regression</p>
<p>Clearly, the observed unmodelled confounding error is not able to match the ranking based on the Original Estimate. It is not even able to tell that the clear winner amongst the methods according to the true value is DML CATE Estimator</p>
</section>
</section>
<section id="Unobserved-confounding-error">
<h1>Unobserved confounding error<a class="headerlink" href="#Unobserved-confounding-error" title="Permalink to this heading"></a></h1>
<p>For each estimator, we now simulate unobserved confounders and check its effect using dummy outcome refuter to check the unobserved confounding error for each estimator. That is, we run the refutation test for each estimator not only on the observed confounder, but also on an unobserved confounder that we simulate using the AddUnobservedCommonCause class and analyse whether there is a strong confounder that is unobserved (missing) and needs to be accounted for.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>unobserved_confounding_error = Experiment(
    experiment_name=&#39;Test&#39;,
    experiment_id=&#39;2&#39;,
    num_experiments=10, # 10
    sample_sizes=sample_size,
    dgps=dgp_list,
    estimators=estimator_tuples,
    refuters=refuter_tuples,
    simulate_unobserved_confounding = True
)

# Run the experiment
res = pd.read_csv(unobserved_confounding_error.experiment())
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>##This plot shows the Mean Absolute Error of the Orginal Estimate from the true value and of the New Effect from
#the expected value for each estimator.
plot_MAEs(res)
</pre></div>
</div>
</div>
<section id="id1">
<h2>Ranking based on Original Estimate<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h2>
<p>The Original Estimate is calculated in the presence of the True Value (that is, the ground truth). However in many real life datasets, the ground truth may not be known. Hence, we want the ranking produced by our refutation tests to be in coherence with that obtained from the Original Estimates. According to the Original Estimate values, the ranking of the estimators should be as follows (the method with the least MAE should get the best rank): 1. DMLCateEstimator 2. Propensity Score Matching 3.
LinearRegression 4. LinearDRLearner</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>#This plot shows the deviation of the original estimate, the new effect and the estimated effect from the true value
refuter = &#39;dummy_outcome_refuter&#39;
deviation_list = []
for estimator in estimator_list:
    plot_estimators_and_refuters(refuter, estimator)
    avg_deviation = ((res[refuter+&#39;:&#39;+estimator+&#39;:NEW_EFFECT&#39;]).sum(axis=0))
    print(avg_deviation)
    deviation_list.append(avg_deviation)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_deviations(estimator_list, deviation_list)
for i in range(len(estimator_list)):
    print(estimator_list[i] +&quot;: &quot;+ str(deviation_list[i]))
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>{k: v for k, v in sorted(zip(estimator_list, deviation_list), key=lambda item: item[1], reverse = True)}
</pre></div>
</div>
</div>
</section>
<section id="id2">
<h2>Ranking based on New Effect (Refutatation results)<a class="headerlink" href="#id2" title="Permalink to this heading"></a></h2>
<p>The ranking based on absolute value of deviations is : 1. DML 2. Linear DR Learner 3. Propensity Score Matching 4. Linear Regression</p>
<section id="We-can-see-that-this-ranking-produces-the-same-top-ranked-estimator-as-the-one-based-on-Original-Estimate.-Thus-ranking-based-on-the-unobserved-confounding-error-solves-the-problem-and-gives-us-a-close-to-correct-ranking-amongst-methods.">
<h3>We can see that this ranking produces the same top-ranked estimator as the one based on Original Estimate. Thus ranking based on the unobserved confounding error solves the problem and gives us a close-to-correct ranking amongst methods.<a class="headerlink" href="#We-can-see-that-this-ranking-produces-the-same-top-ranked-estimator-as-the-one-based-on-Original-Estimate.-Thus-ranking-based-on-the-unobserved-confounding-error-solves-the-problem-and-gives-us-a-close-to-correct-ranking-amongst-methods." title="Permalink to this heading"></a></h3>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, PyWhy contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>